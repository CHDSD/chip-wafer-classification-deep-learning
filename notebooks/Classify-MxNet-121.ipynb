{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook continues from the _DataPrep_ notebook and tries to use a CNN based on ResNet in the MxNet framework for classification.\n",
    "\n",
    "## Data Set\n",
    "\n",
    "[Qingyi](https://www.kaggle.com/qingyi). (February 2018). WM-811K wafer map, Version 1. Retrieved January 2018 from https://www.kaggle.com/qingyi/wm811k-wafer-map/downloads/wm811k-wafer-map.zip/1.\n",
    "\n",
    "## License\n",
    "\n",
    "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, model_zoo\n",
    "from mxnet.gluon import utils as gutils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'vdata'\n",
    "train_imgs = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, 'train'))\n",
    "valid_imgs = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, 'valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, labels, num_rows, num_cols, scale=2):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            axes[i][j].imshow(imgs[i * num_cols + j].asnumpy())\n",
    "            axes[i][j].axes.get_xaxis().set_visible(False)\n",
    "            axes[i][j].axes.get_yaxis().set_visible(False)\n",
    "            axes[i][j].set_title(labels[i * num_cols + j], fontsize=10)\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 examples of each class\n",
    "images_to_show = []\n",
    "labels_to_show = []\n",
    "label_counter = { ii: 0 for ii in range(len(train_imgs.synsets)) }\n",
    "for item in train_imgs:\n",
    "    img = item[0]\n",
    "    label = item[1]\n",
    "    if label_counter[label] > 4:\n",
    "        continue\n",
    "    else:\n",
    "        images_to_show.append(img)\n",
    "        labels_to_show.append(train_imgs.synsets[label])\n",
    "        label_counter[label] = label_counter[label] + 1\n",
    "        \n",
    "ax = show_images(images_to_show, labels_to_show, 9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = gdata.vision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.RandomResizedCrop(224),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    gdata.vision.transforms.RandomFlipTopBottom(),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "valid_augs = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(256),\n",
    "    gdata.vision.transforms.CenterCrop(224),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "learning_rate = 0.001\n",
    "wd = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "\n",
    "ctx = [mx.gpu()]\n",
    "    \n",
    "# Define pretrained model\n",
    "pretrained_net = model_zoo.vision.resnet18_v2(ctx=ctx, pretrained=True)\n",
    "net = model_zoo.vision.resnet18_v2(ctx=ctx, classes=9)\n",
    "net.features = pretrained_net.features\n",
    "net.output.initialize(init.Xavier(), ctx=ctx)\n",
    "net.output.collect_params().setattr('lr_mult', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(\n",
    "    train_imgs.transform_first(train_augs), batch_size, shuffle=True)\n",
    "valid_iter = gdata.DataLoader(\n",
    "    valid_imgs.transform_first(valid_augs), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer is for updating parameters with gradient.\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "net.hybridize()\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', \n",
    "                        optimizer_params={'learning_rate': learning_rate, 'wd': wd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, log_steps=500):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
    "            ls = []\n",
    "            with autograd.record():\n",
    "                y_hats = [net(X) for X in Xs]\n",
    "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
    "            n += sum([l.size for l in ls])\n",
    "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "                                  for y_hat, y in zip(y_hats, ys)])\n",
    "            m += sum([y.size for y in ys])\n",
    "            if i % log_steps == 0 and i > 0:\n",
    "                print(\"Batch {0}: accuracy = {1}\".format(str(i), str(train_acc_sum/m)))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_iter, valid_iter, net, loss, trainer, ctx, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('{}/model-resnet18-121.params'.format('vdata/mxnet'))\n",
    "net.export(\"model_gluon_resnet18_121\", epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, 'test'))\n",
    "test_augs = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(256),\n",
    "    gdata.vision.transforms.CenterCrop(224),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "import itertools\n",
    "def evaluate_metrics(dataset, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    preds = []\n",
    "    trues = []\n",
    "    cnt = 0\n",
    "    cnt_step = 500\n",
    "    for item in dataset:\n",
    "        img = item[0]\n",
    "        label = item[1]    \n",
    "        img = test_augs(img)\n",
    "        img = img.expand_dims(axis=0)\n",
    "        trues.append(label)\n",
    "        \n",
    "        output = net(img.as_in_context(mx.gpu()))\n",
    "        output_label = output.argmax(axis=1).copyto(mx.cpu())  \n",
    "        output_label_cpu = int(output_label.asnumpy()[0])\n",
    "        preds.append(output_label_cpu)  \n",
    "        if cnt % cnt_step == 0:\n",
    "            print(\"Working on \" + str(cnt))\n",
    "        cnt = cnt + 1\n",
    "    return trues, preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues, preds = evaluate_metrics(test_imgs, net, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Accuracy: {0}\".format(accuracy_score(trues, preds)))\n",
    "print(\"Weighted F1 Score: {0}\".format(f1_score(trues, preds, average='weighted')))\n",
    "print(\"Weighted F-beta: {0}\".format(fbeta_score(trues, preds, average='weighted', beta=1.0)))\n",
    "print(\"Macro F1 Score: {0}\".format(f1_score(trues, preds, average='macro')))\n",
    "print(\"Macro F-beta: {0}\".format(fbeta_score(trues, preds, average='macro', beta=1.0)))\n",
    "print(\"Micro F1 Score: {0}\".format(f1_score(trues, preds, average='micro')))\n",
    "print(\"Micro F-beta: {0}\".format(fbeta_score(trues, preds, average='micro', beta=1.0)))\n",
    "print(classification_report(trues, preds, target_names=test_imgs.synsets))\n",
    "cm = confusion_matrix(trues, preds)\n",
    "plot_confusion_matrix(cm, test_imgs.synsets, normalize=False)\n",
    "plot_confusion_matrix(cm, test_imgs.synsets, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "The MxNet model isn't performing as well as the PyTorch version.  Part of that may be due to the `fit one cycle` learning rate scheduler in FastAI.  A few things to try:\n",
    "\n",
    "* Use the MxNet LR scheduler.\n",
    "* Try different base learning rates and weight decays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
